{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: albumentations in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (2.0.5)\n",
      "Requirement already satisfied: matplotlib in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: opencv-python in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: filelock in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albumentations) (2.11.1)\n",
      "Requirement already satisfied: albucore==0.0.23 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (3.12.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Alberto/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchvision albumentations matplotlib opencv-python numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define transformation\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512), # resizing image to 512x512 pixels\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)), # normalizing pixel values ([0, 255]->[-1, 1])\n",
    "    ToTensorV2() # converts images to pytorch tensors\n",
    "])\n",
    "\n",
    "class DropletDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir # path to the folder containing input images\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform # to apply tranformations\n",
    "        self.images = os.listdir(image_dir) # Lists all filenames in image_dir (assuming masks have the same names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given an item index, retrieves its image path and mask path\n",
    "        img_name = self.images[idx]  # Get image filename\n",
    "        mask_name = img_name.rsplit(\".\", 1)[0] + \"_mask.\" + img_name.rsplit(\".\", 1)[1]  # Replace extension correctly\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        image = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB) # reads image in RGB\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # reads mask in greyscale\n",
    "\n",
    "        if image is None or mask is None:\n",
    "            raise ValueError(f\"Failed to load image or mask: {img_path}, {mask_path}\")\n",
    "\n",
    "        if self.transform:\n",
    "            # apply transformations to both image and mask\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        return image, mask.long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(DropletDataset(\"../../dataset_120_real\", \"../segmentation_masks\", transform), batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[4] = torch.nn.Conv2d(256, 1, kernel_size=(1,1))  # Adjust output for binary segmentation\n",
    "#model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target):\n",
    "    smooth = 1.0\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum()\n",
    "    return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
    "\n",
    "criterion = lambda pred, target: 0.5 * F.binary_cross_entropy_with_logits(pred, target.float()) + dice_loss(pred, target)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@447.912] global loadsave.cpp:268 findDecoder imread_('../segmentation_masks/frame_106.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model.train()\n\u001b[32m      5\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# images, masks = images.cuda(), masks.cuda()\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/esilv2_pi2/droplet-ml/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mDropletDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     29\u001b[39m mask_path = os.path.join(\u001b[38;5;28mself\u001b[39m.mask_dir, \u001b[38;5;28mself\u001b[39m.images[idx])\n\u001b[32m     31\u001b[39m image = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB) \u001b[38;5;66;03m# reads image in RGB\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m mask = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMREAD_GRAYSCALE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# reads mask in greyscale\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load image or mask: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        # images, masks = images.cuda(), masks.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]\n",
    "        loss = criterion(outputs.squeeze(1), masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
